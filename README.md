# Emotion-VLLM
Emotion-VLLM: Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation

Abstract  
Facial emotion perception in vision large language model (VLLM) is a crucial element for achieving natural human-machine interaction. Creating high-quality emotion analysis annotations for downstream emotion analysis tasks demands costly expertise and specialized domain knowledge. Likewise, the lack of high-quality emotion analysis instruction data limits the performance of VLLMs in facial emotion perception. Given the issues mentioned above, in this paper, we proposed a self-verification approach with emotion knowledge enhancement (SEFK) to generate high-quality instruction data for facial emotion analysis using VLLM in a cost-effective manner. We integrate high-quality annotations from facial expression recognition (FER), action unit (AU) detection, and valence/arousal estimation (VAE), along with the emotion analysis capabilities of VLLMs, and propose a self-verification method with Uncertainty-Aware Monte Carlos samplings (SV-UAMC) to generate reliable instructions in a low cost manner." Additionally, we released a facial emotion analysis instruction dataset and introduced a facial emotion analysis benchmark to measure the facial emotion perception abilities of VLLM. Extensive experiments have shown the effectiveness of the proposed method for high-quality facial emotion instruction data generation.
